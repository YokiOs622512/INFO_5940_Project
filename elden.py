import streamlit as st
import os
import tempfile
import uuid
import hashlib
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document

# ==========================================
# 1. é…ç½®ä¸åˆå§‹åŒ– (Configuration)
# ==========================================

st.set_page_config(
    page_title="Elden Ring: The Shattered Conversation", 
    page_icon="ğŸ’", 
    layout="wide"
)

# ä½¿ç”¨ Cornell API é…ç½®
# å°è¯•ä»ç¯å¢ƒå˜é‡è·å– Keyï¼Œå¦‚æœæœ¬åœ°æµ‹è¯•æ²¡æœ‰è®¾ç½®ï¼Œè¯·æ‰‹åŠ¨å¡«å…¥
api_key = os.environ.get("API_KEY") 
base_url = "https://api.ai.it.cornell.edu"

if not api_key:
    st.error("âš ï¸ API Key not found. Please set your API_KEY in the environment variables.")
    st.stop()

# åˆå§‹åŒ– LLM (ç”¨äºç”Ÿæˆå¯¹è¯) - ä½¿ç”¨ GPT-4o è·å–æ›´å¥½çš„è§’è‰²æ‰®æ¼”æ•ˆæœ
llm = ChatOpenAI(
    model="openai.gpt-4o",
    temperature=0.7,  # ç¨å¾®è°ƒé«˜æ¸©åº¦ï¼Œè®© NPC å¯¹è¯æ›´ç”ŸåŠ¨
    openai_api_key=api_key,
    openai_api_base=base_url
)

# åˆå§‹åŒ– Embedding (ç”¨äºæ£€ç´¢)
embeddings = OpenAIEmbeddings(
    model="openai.text-embedding-3-small",
    openai_api_key=api_key,
    openai_api_base=base_url
)

# ==========================================
# 2. è§’è‰²è®¾å®š (Persona System) - Innovation
# ==========================================
# è¿™é‡Œå®šä¹‰äº†ä¸åŒ NPC çš„â€œçµé­‚â€ï¼Œé€šè¿‡ System Prompt æ³¨å…¥
NPC_PERSONAS = {
    "Ranni the Witch": {
        "description": "The enigmatic lunar princess.",
        "prompt": """
        You are Ranni the Witch from Elden Ring.
        Tone: Cold, mysterious, archaic, and regal. You often use old English (thee, thou, thy).
        Personality: You seek to overthrow the Golden Order and usher in the Age of Stars. You are a demigod but discarded your Empyrean flesh.
        Constraint: Do NOT act like a robotic assistant. Act entirely as Ranni. If the retrieved context is missing, be vague and mysterious about the stars.
        """
    },
    "Melina": {
        "description": "Your guide and the Kindling Maiden.",
        "prompt": """
        You are Melina, a guide to the Tarnished.
        Tone: Soft-spoken, dutiful, slightly melancholic but supportive.
        Personality: You wish to guide the player to the Erdtree to fulfill your purpose. You often quote Queen Marika's echoes.
        Constraint: Refer to the user as "Tarnished". Keep answers concise but poetic.
        """
    },
    "Iron Fist Alexander": {
        "description": "The jovial Warrior Jar.",
        "prompt": """
        You are Alexander, the Iron Fist! A sentient Warrior Jar.
        Tone: Boisterous, hearty, optimistic, and loud!
        Personality: You seek to become a great champion by stuffing your insides with the remains of warriors.
        Constraint: Use exclamations! Refer to the user as "my friend" or "brave warrior". Talk about getting stuck in holes if relevant.
        """
    }
}

# ==========================================
# 3. çŠ¶æ€ç®¡ç† (Session State)
# ==========================================

if "messages" not in st.session_state:
    # é»˜è®¤ç¬¬ä¸€æ¡æ¶ˆæ¯
    st.session_state.messages = [{"role": "assistant", "content": "Greetings, Tarnished. Which lore fragments shall we explore today?"}]

if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

if "processed_file_hashes" not in st.session_state:
    st.session_state.processed_file_hashes = set()

if "current_persona" not in st.session_state:
    st.session_state.current_persona = "Ranni the Witch"

# ==========================================
# 4. åç«¯é€»è¾‘å‡½æ•° (Backend Logic)
# ==========================================

def create_file_hash(uploaded_file):
    """åˆ›å»ºæ–‡ä»¶å“ˆå¸Œä»¥é¿å…é‡å¤å¤„ç† (æ¥è‡ª chat_with_pdf.py)"""
    content_preview = uploaded_file.getvalue()[:100] if hasattr(uploaded_file, 'getvalue') else b''
    hash_input = f"{uploaded_file.name}_{uploaded_file.size}_{content_preview}"
    return hashlib.md5(hash_input.encode()).hexdigest()

def process_documents(uploaded_files):
    """è¯»å–ä¸Šä¼ çš„ Lore æ–‡ä»¶ (PDF/TXT) å¹¶å­˜å…¥ ChromaDB"""
    if not uploaded_files:
        return False
    
    all_documents = []
    new_files_count = 0
    
    for uploaded_file in uploaded_files:
        file_hash = create_file_hash(uploaded_file)
        
        if file_hash in st.session_state.processed_file_hashes:
            continue
            
        with st.spinner(f"Communing with the Greater Will (Processing {uploaded_file.name})..."):
            text_content = ""
            
            # å¤„ç† PDF
            if uploaded_file.type == "application/pdf":
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name
                try:
                    loader = PyPDFLoader(tmp_file_path)
                    docs = loader.load()
                    text_content = "\n".join([doc.page_content for doc in docs])
                finally:
                    os.unlink(tmp_file_path)
            # å¤„ç† TXT
            elif uploaded_file.type == "text/plain":
                text_content = uploaded_file.read().decode("utf-8")
                
            if text_content:
                # æ·»åŠ å…ƒæ•°æ®
                all_documents.append(Document(
                    page_content=text_content, 
                    metadata={"source": uploaded_file.name}
                ))
                st.session_state.processed_file_hashes.add(file_hash)
                new_files_count += 1
    
    if all_documents:
        # åˆ†å— (Chunking) - è®¾ç½®è¾ƒå°çš„ chunk ä»¥è·å–ç²¾ç¡®çš„ Lore
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
        chunks = text_splitter.split_documents(all_documents)
        
        # å­˜å…¥ ChromaDB
        if st.session_state.vector_store is None:
            st.session_state.vector_store = Chroma.from_documents(
                documents=chunks,
                embedding=embeddings,
                collection_name=f"elden_ring_lore_{uuid.uuid4().hex[:8]}"
            )
        else:
            st.session_state.vector_store.add_documents(chunks)
            
        return True
    return False

def get_chat_history_str(max_history=4):
    """æ ¼å¼åŒ–æœ€è¿‘çš„å¯¹è¯å†å²ï¼Œè®© AI æ‹¥æœ‰è®°å¿†"""
    history = st.session_state.messages[-max_history:]
    history_str = ""
    for msg in history:
        role = "Player" if msg["role"] == "user" else "NPC"
        history_str += f"{role}: {msg['content']}\n"
    return history_str

def generate_npc_response(question, persona_name):
    """æ ¸å¿ƒ RAG é€»è¾‘ï¼šæ£€ç´¢ + è§’è‰²æ‰®æ¼”ç”Ÿæˆ"""
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰çŸ¥è¯†åº“
    if st.session_state.vector_store is None:
        return "Tarnished, you possess no memory fragments (Please upload Lore documents first).", []
    
    # 2. æ£€ç´¢ (Retrieval)
    # æ£€ç´¢ Top 4 ç›¸å…³ç‰‡æ®µ
    docs = st.session_state.vector_store.similarity_search(question, k=4)
    context_text = "\n\n".join([d.page_content for d in docs])
    
    # 3. æ„å»º Prompt
    persona_prompt = NPC_PERSONAS[persona_name]["prompt"]
    chat_history = get_chat_history_str()
    
    full_template = f"""
    {persona_prompt}
    
    You are engaging in a conversation with a player.
    Use the following "Lore Knowledge" to answer the player's question.
    If the answer isn't in the Lore, stay in character and improvise vaguely.
    
    ---
    LORE KNOWLEDGE:
    {context_text}
    ---
    
    CONVERSATION HISTORY:
    {chat_history}
    
    Player: {question}
    Response:
    """
    
    # 4. ç”Ÿæˆå›ç­”
    response = llm.invoke(full_template)
    return response.content, docs

# ==========================================
# 5. å‰ç«¯ç•Œé¢ (Streamlit UI)
# ==========================================

# --- ä¾§è¾¹æ : è®¾ç½®ä¸èµ„æº ---
with st.sidebar:
    st.image("https://upload.wikimedia.org/wikipedia/en/thumb/b/b9/Elden_Ring_Box_Art.jpg/220px-Elden_Ring_Box_Art.jpg", caption="Elden Ring Lore Companion", width=150)
    st.header("âš™ï¸ Game Setup")
    
    # 1. è§’è‰²é€‰æ‹©
    st.subheader("Select NPC")
    selected_persona = st.selectbox(
        "Who do you want to talk to?",
        options=list(NPC_PERSONAS.keys()),
        index=0
    )
    # å¦‚æœåˆ‡æ¢äº†è§’è‰²ï¼Œè®°å½•çŠ¶æ€
    if selected_persona != st.session_state.current_persona:
        st.session_state.current_persona = selected_persona
        # å¯é€‰ï¼šåˆ‡æ¢è§’è‰²æ—¶æ˜¯å¦æ¸…ç©ºå†å²ï¼Ÿä¸ºäº†æ¼”ç¤ºMemoryåŠŸèƒ½ï¼Œå»ºè®®ä¿ç•™ï¼Œæˆ–è€…åŠ ä¸ªToastæç¤º
        st.toast(f"Summoned {selected_persona}!")
    
    st.info(f"**Current Persona:** {NPC_PERSONAS[selected_persona]['description']}")
    
    st.divider()
    
    # 2. æ–‡ä»¶ä¸Šä¼ 
    st.subheader("ğŸ“œ Lore Fragments (Knowledge Base)")
    uploaded_files = st.file_uploader(
        "Upload PDF/TXT Lore", 
        type=["pdf", "txt"], 
        accept_multiple_files=True
    )
    
    if uploaded_files:
        if process_documents(uploaded_files):
            st.success("âœ… Fragments processed into Memory!")
            
    # æ˜¾ç¤ºçŠ¶æ€
    if st.session_state.vector_store:
        doc_count = st.session_state.vector_store._collection.count()
        st.markdown(f"*Current Memory Fragments: {doc_count} chunks*")
    
    st.divider()
    if st.button("ğŸ—‘ï¸ Reset World (Clear All)"):
        st.session_state.clear()
        st.rerun()

# --- ä¸»ç•Œé¢: èŠå¤©çª—å£ ---
st.title("Elden Ring: The Shattered Conversation")
st.caption(f"Talking to: **{st.session_state.current_persona}** | Powered by RAG & OpenAI")

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    # è‡ªå®šä¹‰å¤´åƒ
    avatar = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ§™â€â™€ï¸"
    if msg["role"] == "assistant":
        if "Alexander" in st.session_state.current_persona: avatar = "ğŸº"
        elif "Melina" in st.session_state.current_persona: avatar = "ğŸ”¥"
    
    st.chat_message(msg["role"], avatar=avatar).write(msg["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("Speak thy mind, Tarnished..."):
    # 1. è®°å½•å¹¶æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user", avatar="ğŸ‘¤").write(prompt)
    
    # 2. ç”Ÿæˆå›å¤
    with st.chat_message("assistant", avatar="ğŸ§™â€â™€ï¸"):
        with st.spinner(f"{st.session_state.current_persona} is pondering the stars..."):
            response_text, source_docs = generate_npc_response(prompt, st.session_state.current_persona)
            
            st.markdown(response_text)
            
            # 3. å¤æ‚æ€§å±•ç¤º (Complexity): æ˜¾ç¤ºæ€ç»´é“¾/æ¥æº
            if source_docs:
                with st.expander("ğŸ”® See NPC's Thoughts & Lore Source"):
                    st.markdown("**Reasoning:** The NPC retrieved these fragments to answer you:")
                    for i, doc in enumerate(source_docs):
                        st.markdown(f"**Fragment {i+1} (from {doc.metadata.get('source')}):**")
                        st.caption(doc.page_content[:300] + "...") # åªæ˜¾ç¤ºå‰300å­—
    
    # 4. è®°å½• AI å›å¤åˆ°å†å²
    st.session_state.messages.append({"role": "assistant", "content": response_text})
